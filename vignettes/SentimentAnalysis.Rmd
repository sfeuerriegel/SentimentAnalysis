---
title: "SentimentAnalysis Vignette"
author: 
- "Stefan Feuerriegel"
- "Nicolas Proellochs"
date: "`r Sys.Date()`"
bibliography: bibliography.bib
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Introduction to SentimentAnalysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The `SentimentAnalysis` package introduces a powerful toolchain facilitating the sentiment analysis of textual contents in R. This implementation utilizes various existing dictionaries, such as General Inquirer, Harvard IV or Loughran-McDonald. Furthermore, it can also create customized dictionaries. The latter uses LASSO regularization as a statistical approach to select relevant terms  based on an exogeneous response variable. Finally, all methods can be easility compared using built-in evaluation routines.

# Introduction

Sentiment analysis is a research branch located at the heart of natural language processing (NLP), computational linguistics and text mining. It refers to any measure that extract subjective information from textual documents. In other words, it extracts the polarity of the expressed opinion in the range from being positive to negative. As a result, one refers to sentiment analysis also as *opinion mining* [@Pang.2008].

## Applications in research

Sentiment analysis has received great traction lately [@Ravi.2015; @Pang.2008], which we explore in the following. Current research in finance and the social sciences utilizes sentiment analysis to understand human decisions in response to textual materials. This immediately reveals manifold implications for practitioners, as well as those involved in the fields of finance research and the social sciences: researchers can use R to extract text components that are relevant for readers and test their hypotheses on this basis. By the same token, practitioners can measure which wording actually matters to their readership and enhance their writing accordingly [@ECIS.2015]. We demonstrate the added benefits in two case studies drawn from finance and the social sciences below.

## Applications in practice

Several applications demonstrate the use cases of sentiment analysis for organizations and enterprises:

* **Finance:** Investors in financial markets refer to textual information in the form of financial news disclosures before exercising ownership in stocks. Interestingly, they not only rely on quantitiative numbers, but also soft information strongly influences stock prices, such as tone and sentiment [@Henry.2008; @Loughran.2011; @Tetlock.2007]. Automated traders then can automatically analyze the sentiment conveyed in financial disclosures in order to trigger investment decisions within milliseconds. 

* **Marketing:** Marketing departments are often interested in tracking the brand image. For that purpose, they collect large volumes of user opinions from social media and evaluate the opinions of individuals towards brands, products and services. Furthermore, practioners from marketing can exploit the insights to enhance their wording according to the feedback of their readingship.

* **Rating and review platforms:** Rating and review platforms support individuals by collecting user ratings or preferences towards products and services. Here, one can automatically process large volumes of user-generated content and exploit the knowledge there it. For example, one can identify which cues convey a positive and and negative opinion, or even automatically validate their credibility. 

# Methods for sentiment analysis

As sentiment analysis is applied to a broad variety of domains and textual sources, research has devised various approaches to measure sentiment. A recent literature overview [@Pang.2008] provides a comprehensive domain-independent survey. 

On the one hand, machine learning approaches are prefered when one strives for a high prediction performance. However, machine learning usually works as a black-box; hence, making interpretations diffucult. On the other hand, dictionary-based approaches facilitate lists of positive and negative words. Their occurences are then combined into a single sentiment score. Therefore, the underlying decisions become traceable and researchers can understand the reasons resulting into a specific sentiment. 

In addition to that, `SentimentAnalysis` allows to generate tailored dictionaries. These are customized to a specific domain, improve the prediction performance compared to pure dictionaries and allow full interpretability. Details for this methodology can be found in [@ECIS.2015]. 

On the road to performing sentiment analysis, one needs to convert the running text into a machine readable format. This is achieved by executing a series of preprocessing operations. First, the text is tokenized into single words, followed by what are common preprocessing steps: stopword removal, stemming, removal of punctuation and conversion to lower-case. These are also conducted by default in the `SentimentAnalysis`, but can be adapted to ones personal needs.

# Setup of the SentimentAnalysis package

Even though sentiment analysis has received great traction lately, the available tools are not yet living up to the needs of researchers. The `SentimentAnalysis` package is intended to partially close this gap and contributed capabilities that most research desires.

## Installation

Using the `devtools` package, you can easily install the latest development version of `SentimentAnalysis` with

```{r,eval=FALSE}
library(devtools)

# Option 1: download and install latest version from GitHub
install_github("sfeuerriegel/SentimentAnalysis")

# Option 2: install directly from bundled archive
install_local("SentimentAnalysis_1.0.0.tar.gz")
```

Note: that you have to specify the path either to the directory of `SentimentAnalysis` or to the bundled archive `SentimentAnalysis_1.0.0.tar.gz`.

## Package loading

Afterwards, one merely needs to load the `SentimentAnalysis` package as follows. This section shows the basic functionality to crawl for ad hoc filings. The following lines extract the ad hoc disclosure that was published the latest. 

```{r}
library(SentimentAnalysis)
```

# Brief demonstration

```{r}
# Analyze a single string to obtain a binary response (positive / negative)
sentiment <- analyzeSentiment("Yeah, this was a great soccer game of the German team!")
convertToBinaryResponse(sentiment)$SentimentGI
```

```{r}
# Create a vector of strings
documents <- c("Wow, I really like the new light sabers!",
               "That book was excellent.",
               "R is a fantastic language.",
               "The service in this restaurant was miserable.",
               "This is neither positive or negative.",
               "The waiter forget about my a dessert -- what a poor service!")

# Analyze sentiment
sentiment <- analyzeSentiment(documents)

# Extract dictionary-based sentiment according to the Harvard-IV dictionary
sentiment$SentimentGI

# View sentiment direction (i.e. positive, neutral and negative)
convertToDirection(sentiment$SentimentGI)

response <- c(+1, +1, +1, -1, 0, -1)

compareToResponse(sentiment, response)

compareToResponse(sentiment, convertToBinaryResponse(response))

plotSentimentResponse(sentiment$SentimentGI, response)
```

The `SentimentAnalysis` package works very clever and neatly here in order to remove the effort
for the user: it recognizes that the user has inserted
a vector of strings and thus automatically performs a set of default preprocessing operations from
text mining. Hence, it tokenizes each document and finally converts the input into a 
document-term matrix. All of the previous operations are undertaken without manual specification. 
The `analyzeSentiment()` routine also accepts other input formats in case the user has already
performed a preprocessing or wants to specify a specific set of operations.

# Functionality

The following sections present the functionality to work with different input formats and the underlying dictionaries.

## Interface

The `SentimentAnalysis` package provides interfaces to several other input formats, among these are

* Vector of strings.

* DocumentTermMatrix and TermDocumentMatrix as implemented in the `tm` package [@Feinerer.2008].

* Corpus object as implemented by the `tm` package [@Feinerer.2008].

We provide examples in the following.

### Vector of strings

```{r}
documents <- c("This is good",
               "This is bad",
               "This is inbetween")
convertToDirection(analyzeSentiment(documents)$SentimentGI)
```

### Document-term matrix

```{r}
library(tm)
corpus <- Corpus(VectorSource(documents))
convertToDirection(analyzeSentiment(corpus)$SentimentGI)
```

### Corpus object

```{r}
dtm <- preprocessCorpus(corpus)
convertToDirection(analyzeSentiment(dtm)$SentimentGI)
```

Since the package can directly work with a document-term matrix, this allows to use customized preprocessing operations in the first place. Afterwards, one can utilize the `SentimentAnalysis` package for the computation of sentiment scores. For instance, one can replace the stopwords by a different list, or even perform tailored synonym merging among others. By default, the package uses the built-in routines `transformIntoCorpus()` to convert the input into a `Corpus` object and `preprocessCorpus()` to convert it into a `DocumentTermMatrix`.

## Built-in dictionaries

The `SentimentAnalysis` package entails three different dictionaries:

* Harvard-IV dictionary as used in the General Inquirer program

* Henry's Financial dictionary [@Henry.2008]

* Loughran-McDonald Financial dictionary [@Loughran.2011]

All of them can be manually inspected and even accessed as follows:

```{r}
# Make dictionary available in the current R environment
data(DictionaryGI)
# Display the internal structure 
str(DictionaryGI)
# Access dictionary as an object of type SentimentDictionary
dict.GI <- loadDictionaryGI()
# Print summary statistics of dictionary
summary(dict.GI)

data(DictionaryHE)
str(DictionaryHE)

data(DictionaryLM)
str(DictionaryLM)
```

## Dictionary functions

The `SentimentAnalysis` packgae distinguishes between three different types of dictionaries. All of them differ by the data they store, which finally also controls what methods for sentiment analysis one can apply. The dictionaries are as follows:

* `SentimentDictionaryWordlist` contains a list of words belonging to a single category. For instance, it can bundle a list of uncertainty words in order to compute the ratio of uncertainty words in that documents. 

* `SentimentDictionaryBinary` stores two list of words, one for positive and one for negative entries. This allows to later compute the polarity of the document on a scale from very positive to very negative. However, each category is not further distinguished or rated, i.e. all positive words are assigned the same degree of positivity. 

* **SentimentDictionaryWeighted** allows words to take on continuous sentiment scores. This allows, for instance, to rate *increase* as being more positive than *improve*. These weights can then be transformed into a linear model. For that purpose, the **SentimentDictionaryWeighted** also entails an intercept. It can also store an additional factor in order to revert the weighting by an inverse document frequency. 

### SentimentDictionaryWordlist

```{r}
d <- SentimentDictionaryWordlist(c("uncertain", "possible", "likely"))
summary(d)

# Alternative call
d <- SentimentDictionary(c("uncertain", "possible", "likely"))
summary(d)
```

### SentimentDictionaryBinary

```{r}
d <- SentimentDictionaryBinary(c("increase", "rise", "more"),
                               c("fall", "drop"))
summary(d)

# Alternative call
d <- SentimentDictionary(c("increase", "rise", "more"),
                         c("fall", "drop"))
summary(d)
```

### SentimentDictionaryWeighted

```{r}
d <- SentimentDictionaryWeighted(c("increase", "decrease", "exit"),
                                 c(+1, -1, -10),
                                 rep(NA, 3))
summary(d)

# Alternative call
d <- SentimentDictionary(c("increase", "decrease", "exit"),
                         c(+1, -1, -10),
                         rep(NA, 3))
summary(d)                         
```

# Dictionary generation

The following example shows how the `SentimentAnalysis`package can extract statistically relevant textual drivers based on an exogeneous response variable. The details of this methods are presented in [@ECIS.2015], while we provide a brief summary here. Let \eqn{R} denote a response variable in form of a vector. Furthermore, variables \eqn{w_1, \ldots, w_n} give the number of occurences of word \eqn{w_i} in a document. The methodology then estimates a linear model \deqn{R = \alpha + \beta_1 w_1 + \ldots + \beta_n w_n} with intercept \eqn{\alpha} and coefficients \eqn{\beta_1, \ldots, \beta_n}. The estimation routine is based on LASSO regularization which implicitly performs variable selection. Thereby, it sets some of the coefficients \eqn{\beta_i} to exactly zero. The remaining words can then be ranked by the polarity according to their coefficient.

```{r}
# Create a vector of strings
documents <- c("This is a good thing!",
               "This is a very good thing!",
               "This is okay.",
               "This is a bad thing.",
               "This is a very bad thing.")
response <- c(1, 0.5, 0, -0.5, -1)

# Generate dictionary with LASSO regularization
dict <- generateDictionary(documents, response)

dict

summary(dict)
```

In practice, users have several options for fine-tuning. Among them, they can disable the intercept \eqn{\alpha} and fix it to zero, or standardizing the response variable \eqn{R}. In addition, it is possible to replace the LASSO by any variant of the elastic net; simply by changing the argument `alpha`. 

Finally, one can save and reload dictionaries using `read()` and `write()` as follows:

```{r,eval=FALSE}
write(dict, file="dictionary.dict")
dict <- read("dictionary.dict")
```


## Performance evaluation

Ultimately, several routines allow to exlore the generated dictionary further. On the hand, a simple overview can be displayed by means of the `summary()`routine. On the other hand, a Kernel Density Estimation can also visualize the distribution of positive and negative words. For instance, one can identify if the opinionated words were skewed to either end of the polarity scale. Lastly, the `compareDictionary()` routine can compare the generated dictionary to dictionaries from the literature. It automatically computes various metrics, among them, the overlap or the correlation. 

```{r}
compareDictionaries(dict,
                    loadDictionaryGI())

sentiment <- predict(dict, documents)
compareToResponse(sentiment, response)
plotSentimentResponse(sentiment, response)
```

The following example demonstrates how a calculated dictionary can then be used for predicting the sentiment on out-of-sample data. In addition, the code then evaluates the prediction performance by comparing it to the built-in dictionaries.

```{r}
test_documents <- c("This is neither good nor bad",
                    "What a good idea!",
                    "Not bad")
test_response <- c(0, 1, 1)

pred <- predict(dict, test_documents)

compareToResponse(pred, test_response)
plotSentimentResponse(pred, test_response)

compareToResponse(analyzeSentiment(test_documents), test_response)
```

## Configuration of preprocessing

When desired, one can implemented a tailored preprocessing stage that adapts to specific needs. The following code snippets demonstrates such adaptation. In particular, the `SentimentAnalysis`` package ships a function `ngram_tokenize()` in order to extract \eqn{n}-grams from the corpus. This does not affect the results of the built-in dictionaries, but changes the behavior of dictionary generation.

```{r}
corpus <- Corpus(VectorSource(documents))
tdm <- TermDocumentMatrix(corpus, 
                          control=list(wordLengths=c(1,Inf), 
                                       tokenize=function(x) ngram_tokenize(x, char=FALSE, 
                                                                           ngmin=1, ngmax=2)))
rownames(tdm)

dict <- generateDictionary(tdm, response)
summary(dict)
dict
```

## Performance optimization

Once the user has decided upon a preferred rule, he can adapt the `analyzeSentiment()` routine by restricting it to calculate only the rules of interest. Such behavior can be accomplished by changing the default value of the argument `rules`. See the following code snippets for an example:

```{r}
sentiment <- analyzeSentiment(documents,
                              rules=list("SentimentLM"=list(ruleSentiment, loadDictionaryLM())))
sentiment
```

## Language support and extensibility

**SentimentAnalyiss** can be adapted for the use with languages other than English. For that, one needs to introduce changes at two points:

* **Preprocessing:** The built-in routines use a parameter `language="english"` to perform all preprocessing operations for the English language. Instead, one might prefer to change stemming and stopwords to the specific language. If one wants to make further changes to the preprocessing, it might be beneficial to replace the automatic preprocessing by one's own routines which then return a `DocumentTermMatrix`.

* **Dictionary:** If one has a response or baseline variable, one can use from the dictionary generation approach that is shipped with `SentimentAnalysis`. This can then automatically generate a dictionary of positive and negative words that can be applied to the given language. Otherwise, if one has not baseline variable at hand, one needs to load a dictionary for that langauge. It might be worthwhile to search online for pre-defined lists of positive and negative words.

The following example demonstrates how `SentimentAnalysis` can be adapted to work with an example in German. Here, we supply a positive and negative document in the variable `documents`. Afterwards, we introduce a very small dictionary of positive and negative words, that is stored in `dictionaryGerman`. Finally, we use `analyzeSentiment()` to perform a sentiment analysis, where we introduce to changes as follows: on the hand, we supply `language="german"` to switch all preprocessing operations being made for the German language. On the other hand, we define our custom rule for `GermanSentiment` that uses our previous, customized dictionary. 

```{r}
documents <- c("Das ist ein gutes Resultat",
               "Das Ergebnis war schlecht")
dictionaryGerman <- SentimentDictionaryBinary(c("gut"), 
                                              c("schlecht"))

sentiment <- analyzeSentiment(documents,
                              language="german",
                              rules=list("GermanSentiment"=list(ruleSentiment, dictionaryGerman)))
sentiment

convertToBinaryResponse(sentiment$GermanSentiment)
```

Notes: 

* The argument `rules` is a named list of approaches, where each entry specifies a combination of a rule and a dictionary.

* Caution is needed when working with stemming. The default routines of `SentimentAnalysis` automatically perform stemming. Therefore, it is necessary to included stemmed terms in the original dictionary. Once can easily achieve such a conversion by calling `tm::stemDocument()`.

# Worked examples

The following example shows the usage of `SentimentAnalysis`in an applied setting. More precisely, we utilize Reuters oil-related news from the `tm` package.

```{r}
library(tm)
data(crude)

# Analyze sentiment
sentiment <- analyzeSentiment(crude)

# Count positive and negative news releases
table(convertToBinaryResponse(sentiment$SentimentLM))

# News releases with highest and lowest sentiment
crude[[which.max(sentiment$SentimentLM)]]$meta$heading
crude[[which.min(sentiment$SentimentLM)]]$meta$heading

# View summary statistics of sentiment variable
summary(sentiment$SentimentLM)

# Visualize distribution of standardized sentiment variable
hist(scale(sentiment$SentimentLM))

# Compute cross-correlation 
cor(sentiment[, c("SentimentLM", "SentimentHE", "SentimentGI")])

# crude oil news between  1987-02-26 until 1987-03-02
datetime <- do.call(c, lapply(crude, function(x) x$meta$datetimestamp))

plotSentiment(sentiment$SentimentLM)
plotSentiment(sentiment$SentimentLM, x=datetime, cumsum=TRUE)
```

# Outlook

The current version still opens avenues for further enhancement. In the future, we see the following items as subject to improvements:

* **Negations:** We envision a generic negation rule object, that can be injected to negate fixed windows or apply complex negation rules [@DSS.2016].

* **Multi-language support:** The current version has built-in dictionaries for the English language only. We think that the package can greatly benefit from support of further languages. As such, one does not need to adapt the preprocessing routines as the underlying `tm` package has already support for further languages [@Feinerer.2008]. Instead, it is only required to tailored dictionaries.

We cordially invite everyone to contribute source code, dictionaries and further demos. 

# License

**SentimentAnalysis** is released under the [MIT License](https://opensource.org/licenses/MIT)
Copyright (c) 2016 Stefan Feuerriegel & Nicolas PrÃ¶llochs

# References
